{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corra\\Documents\\GitHub\\Machine_Learning_Project\\frameworks\\tfkeras\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>data_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>data_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>data_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>data_22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>data_27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>data_416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>data_426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>data_428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>data_430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>data_432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  col1  col2  col3  col4  col5  col6        id\n",
       "NaN       1     1     1     1     1     3     1    data_5\n",
       "NaN       1     1     1     1     1     3     2    data_6\n",
       "NaN       1     1     1     1     3     2     1   data_19\n",
       "NaN       1     1     1     1     3     3     2   data_22\n",
       "NaN       1     1     1     2     1     2     1   data_27\n",
       "..      ...   ...   ...   ...   ...   ...   ...       ...\n",
       "NaN       1     3     3     2     1     4     2  data_416\n",
       "NaN       1     3     3     2     3     1     2  data_426\n",
       "NaN       1     3     3     2     3     2     2  data_428\n",
       "NaN       1     3     3     2     3     3     2  data_430\n",
       "NaN       1     3     3     2     3     4     2  data_432\n",
       "\n",
       "[124 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "print(os.getcwd())\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer=os.getcwd()+'/../../data/monks/monks-1.train',\n",
    "    names=['target', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'id'],\n",
    "    delimiter=' '\n",
    ")\n",
    "\n",
    "# Display the loaded data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1_1</th>\n",
       "      <th>col1_2</th>\n",
       "      <th>col1_3</th>\n",
       "      <th>col2_1</th>\n",
       "      <th>col2_2</th>\n",
       "      <th>col2_3</th>\n",
       "      <th>col3_1</th>\n",
       "      <th>col3_2</th>\n",
       "      <th>col4_1</th>\n",
       "      <th>col4_2</th>\n",
       "      <th>col4_3</th>\n",
       "      <th>col5_1</th>\n",
       "      <th>col5_2</th>\n",
       "      <th>col5_3</th>\n",
       "      <th>col5_4</th>\n",
       "      <th>col6_1</th>\n",
       "      <th>col6_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     col1_1  col1_2  col1_3  col2_1  col2_2  col2_3  col3_1  col3_2  col4_1  \\\n",
       "NaN     1.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0     1.0   \n",
       "NaN     1.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0     1.0   \n",
       "NaN     1.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0   \n",
       "NaN     1.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0     0.0   \n",
       "NaN     1.0     0.0     0.0     1.0     0.0     0.0     0.0     1.0     1.0   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "NaN     0.0     0.0     1.0     0.0     0.0     1.0     0.0     1.0     1.0   \n",
       "NaN     0.0     0.0     1.0     0.0     0.0     1.0     0.0     1.0     0.0   \n",
       "NaN     0.0     0.0     1.0     0.0     0.0     1.0     0.0     1.0     0.0   \n",
       "NaN     0.0     0.0     1.0     0.0     0.0     1.0     0.0     1.0     0.0   \n",
       "NaN     0.0     0.0     1.0     0.0     0.0     1.0     0.0     1.0     0.0   \n",
       "\n",
       "     col4_2  col4_3  col5_1  col5_2  col5_3  col5_4  col6_1  col6_2  \n",
       "NaN     0.0     0.0     0.0     0.0     1.0     0.0     1.0     0.0  \n",
       "NaN     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "NaN     0.0     1.0     0.0     1.0     0.0     0.0     1.0     0.0  \n",
       "NaN     0.0     1.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "NaN     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0  \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "NaN     0.0     0.0     0.0     0.0     0.0     1.0     0.0     1.0  \n",
       "NaN     0.0     1.0     1.0     0.0     0.0     0.0     0.0     1.0  \n",
       "NaN     0.0     1.0     0.0     1.0     0.0     0.0     0.0     1.0  \n",
       "NaN     0.0     1.0     0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "NaN     0.0     1.0     0.0     0.0     0.0     1.0     0.0     1.0  \n",
       "\n",
       "[124 rows x 17 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing labels/targets vector\n",
    "y = df['target'].values.astype(float)\n",
    "\n",
    "# Drop of the IDs and the targets\n",
    "df = df.drop(columns=['target','id'], axis=1)\n",
    "\n",
    "# Initializing the one-hot DataFrame\n",
    "df_copy = df.copy(deep=True)\n",
    "\n",
    "# Iteration on the columns of the DataFrame\n",
    "for column in df.columns:\n",
    "\n",
    "    # Creation of the one-hot encoding's columns\n",
    "    df_one_hot = pd.get_dummies(df[column], dtype=float)\n",
    "\n",
    "    # Change the name of the columns\n",
    "    df_one_hot = df_one_hot.set_axis([column+'_'+str(col) for col in df_one_hot.columns], axis=1)\n",
    "\n",
    "    # Drop of the initial column\n",
    "    df_copy.drop(labels=column, axis=1, inplace=True)\n",
    "\n",
    "    # Concatenation of the new columns to the DataFrame\n",
    "    df_copy = pd.concat([df_copy,df_one_hot], axis=1)\n",
    "\n",
    "\n",
    "# Columns to drop\n",
    "#columns_to_drop = ['col1_2', 'col1_3',\t'col2_2', 'col2_3',  'col3_2',  'col4_2', 'col4_3', 'col5_2', 'col5_3', 'col5_4', 'col6_2']\n",
    "\n",
    "# Drop the specified columns\n",
    "#df_copy = df_copy.drop(columns=columns_to_drop)\n",
    "\n",
    "# Print of the obtained DataFrame   col1_2\tcol1_3\t col2_2\tcol2_3  col3_2\tcol3_3  col4_2\tcol4_3\tcol5_2\tcol5_3 col6_2\t\t\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters' space for Grid Search     \n",
    "param_space = {\n",
    "    'units': [3, 4],\n",
    "    'optimizer': ['sgd'],\n",
    "    'learning_rate': [0.2, 0.3],\n",
    "    'batch_size': [128],\n",
    "    'epochs': [400],\n",
    "    'weight_decay': [0.001,0.002],\n",
    "    'momentum': [0.5, 0.6],\n",
    "    'loss': ['binary_crossentropy'],\n",
    "    'activation': ['tanh'],\n",
    "    'output_activation': ['sigmoid'],\n",
    "    'metrics': ['accuracy']\n",
    "}\n",
    "\n",
    "# Parameters' space for Grid Search (accuracy=0.89)\n",
    "#param_space = {\n",
    "    #'units': [3, 4],\n",
    "    #'optimizer': ['sgd'],\n",
    "    #'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    #'batch_size': [15, 16, 17],\n",
    "    #'epochs': [1250, 1500, 1750],\n",
    "    #'weight_decay': [0.005, 0.01],\n",
    "    #'momentum': [0.7, 0.8, 0.9],\n",
    "    #'loss': ['binary_crossentropy'] #['log_loss', 'binary_crossentropy']\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\corra\\Documents\\GitHub\\Machine_Learning_Project\\frameworks\\tfkeras\\corrado.ipynb Cella 6\u001b[0m line \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m],)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Sets the Early Stopping for the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Training of the model with validation split\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     x\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m,  \u001b[39m# 25% of the data will be used for validation\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Evaluate on both training and validation sets\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/corra/Documents/GitHub/Machine_Learning_Project/frameworks/tfkeras/corrado.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m loss, training_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X, y, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:1856\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1841\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1842\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1843\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1854\u001b[0m         pss_evaluation_shards\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1855\u001b[0m     )\n\u001b[1;32m-> 1856\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1857\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1858\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1859\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1860\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1861\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1862\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1863\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1864\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1865\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1866\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1867\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1868\u001b[0m )\n\u001b[0;32m   1869\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1870\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1871\u001b[0m }\n\u001b[0;32m   1872\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:2289\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_tuner\u001b[39m.\u001b[39mstart()\n\u001b[0;32m   2285\u001b[0m \u001b[39mfor\u001b[39;00m (\n\u001b[0;32m   2286\u001b[0m     _,\n\u001b[0;32m   2287\u001b[0m     dataset_or_iterator,\n\u001b[0;32m   2288\u001b[0m ) \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m-> 2289\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_metrics()\n\u001b[0;32m   2290\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2291\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:2723\u001b[0m, in \u001b[0;36mModel.reset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2704\u001b[0m \u001b[39m\"\"\"Resets the state of all the metrics in the model.\u001b[39;00m\n\u001b[0;32m   2705\u001b[0m \n\u001b[0;32m   2706\u001b[0m \u001b[39mExamples:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2720\u001b[0m \n\u001b[0;32m   2721\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2722\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics:\n\u001b[1;32m-> 2723\u001b[0m     m\u001b[39m.\u001b[39;49mreset_state()\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\metrics\\base_metric.py:265\u001b[0m, in \u001b[0;36mMetric.reset_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_states()\n\u001b[0;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     backend\u001b[39m.\u001b[39;49mbatch_set_value([(v, \u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariables])\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:4311\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   4309\u001b[0m     \u001b[39mfor\u001b[39;00m x, value \u001b[39min\u001b[39;00m tuples:\n\u001b[0;32m   4310\u001b[0m         value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(value, dtype\u001b[39m=\u001b[39mdtype_numpy(x))\n\u001b[1;32m-> 4311\u001b[0m         _assign_value_to_variable(x, value)\n\u001b[0;32m   4312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4313\u001b[0m     \u001b[39mwith\u001b[39;00m get_graph()\u001b[39m.\u001b[39mas_default():\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:4359\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4356\u001b[0m     variable\u001b[39m.\u001b[39massign(d_value)\n\u001b[0;32m   4357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4358\u001b[0m     \u001b[39m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4359\u001b[0m     variable\u001b[39m.\u001b[39;49massign(value)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1056\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m   1054\u001b[0m   validate_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_shape \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape\u001b[39m.\u001b[39mis_fully_defined()\n\u001b[0;32m   1055\u001b[0m   kwargs[\u001b[39m\"\u001b[39m\u001b[39mvalidate_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m validate_shape\n\u001b[1;32m-> 1056\u001b[0m assign_op \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39massign_variable_op(\n\u001b[0;32m   1057\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, value_tensor, name\u001b[39m=\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1058\u001b[0m \u001b[39mif\u001b[39;00m read_value:\n\u001b[0;32m   1059\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy_read(assign_op)\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:149\u001b[0m, in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, validate_shape, name)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    148\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    150\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mAssignVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, value, \u001b[39m\"\u001b[39;49m\u001b[39mvalidate_shape\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    151\u001b[0m       validate_shape)\n\u001b[0;32m    152\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    153\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assigning new DataFrame to Data set variable\n",
    "X = df_copy.values\n",
    "\n",
    "# Number of trials with random combinations\n",
    "num_trials = 15\n",
    "for i in range(num_trials):\n",
    "    \n",
    "    # Random parameters\n",
    "    params = {\n",
    "        'units': np.random.choice(param_space['units']),\n",
    "        'optimizer': np.random.choice(param_space['optimizer']),\n",
    "        'learning_rate': np.random.choice(param_space['learning_rate']),\n",
    "        'batch_size': np.random.choice(param_space['batch_size']),\n",
    "        'epochs': np.random.choice(param_space['epochs']),\n",
    "        'weight_decay': np.random.choice(param_space['weight_decay']),\n",
    "        'momentum': np.random.choice(param_space['momentum']),\n",
    "        'loss': np.random.choice(param_space['loss']),\n",
    "        'activation': np.random.choice(param_space['activation']),\n",
    "        'output_activation': np.random.choice(param_space['output_activation']),\n",
    "        'metrics': np.random.choice(param_space['metrics']),\n",
    "    }\n",
    "\n",
    "    # Building the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Trial  5: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000002568DBB0130>, Learning Rate=0.1, Weight Decay=0.5, Epochs=40, Batch Size=8, Momentum=0.7 Training Accuracy=0.7016, Validation Accuracy=0.7097\n",
    "    # Trial  9: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x00000256895602E0>, Learning Rate=0.2, Weight Decay=0.7, Epochs=20, Batch Size=8, Momentum=0.7 Training Accuracy=0.6855, Validation Accuracy=0.6452\n",
    "    # Trial  0: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x0000025688E4F850>, Learning Rate=0.2, Weight Decay=0.5, Epochs=10, Batch Size=8, Momentum=0.7 Training Accuracy=0.6532, Validation Accuracy=0.8065\n",
    "    # Trial 11: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x0000025698CD55B0>, Learning Rate=0.2, Weight Decay=0.5, Epochs=10, Batch Size=8, Momentum=0.7 Training Accuracy=0.6935, Validation Accuracy=0.7419\n",
    "    # Trial  8: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x0000025698C31460>, Learning Rate=0.2, Weight Decay=0.5, Epochs=20, Batch Size=8, Momentum=0.7 Training Accuracy=0.7339, Validation Accuracy=0.7419\n",
    "    # Trial  3: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x0000019E9C1534C0>, Learning Rate=0.001, Weight Decay=3, Epochs=100, Batch Size=8, Momentum=0.7 Training Accuracy=0.7500, Validation Accuracy=0.6452\n",
    "    # Trial 10: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000001C070661F40>, Learning Rate=0.001, Weight Decay=6, Epochs=150, Batch Size=16, Momentum=0.7 Training Accuracy=0.7742, Validation Accuracy=0.6452\n",
    "    # Trial  2: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000001D72870C880>, Learning Rate=0.0001, Weight Decay=6, Epochs=300, Batch Size=128, Momentum=0.7 Training Accuracy=0.6290, Validation Accuracy=0.7419\n",
    "    # Trial  8: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000001D72A3F2CA0>, Learning Rate=0.0001, Weight Decay=6, Epochs=300, Batch Size=128, Momentum=0.7 Training Accuracy=0.7177, Validation Accuracy=0.6129\n",
    "    # Trial 13: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000001D726D57EB0>, Learning Rate=0.0001, Weight Decay=7, Epochs=300, Batch Size=64, Momentum=0.05 Training Accuracy=0.7016, Validation Accuracy=0.7097\n",
    "    # Trial  9: Units=3, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000001D72B51B0D0>, Learning Rate=0.0001, Weight Decay=8, Epochs=400, Batch Size=128, Momentum=0.05 Training Accuracy=0.6935, Validation Accuracy=0.6452\n",
    "    # Trial 14: Units=3, Act: relu, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000001AAA5248490>, Learning Rate=0.2, Weight Decay=0.001, Epochs=400, Batch Size=128, Momentum=0.6 Training Accuracy=0.7339, Validation Accuracy=0.7742\n",
    "    # Trial  0: Units=4, Act: tanh, Optimizer: <keras.src.optimizers.sgd.SGD object at 0x000001AA9F21AEB0>, Learning Rate=0.3, Weight Decay=0.001, Epochs=400, Batch Size=128, Momentum=0.6 Training Accuracy=0.6855, Validation Accuracy=0.6452\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Add fully connected units to the NN\n",
    "    #model.add(Dense(units=params['units'], activation=params['activation'], kernel_regularizer=regularizers.l2(params['weight_decay']), use_bias=True))\n",
    "    model.add(Dense(units=1, activation=params['output_activation'], use_bias=True))\n",
    "    \n",
    "    # Set the optimizer with the sampled learning rate\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif params['optimizer'] == 'sgd':\n",
    "        optimizer = optimizers.SGD()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    # Sets the Loss Function, the Optimizer used in the model and the Metrics used for evaluation\n",
    "    model.compile(loss=params['loss'], optimizer=\"sgd\", metrics=params['metrics'],)\n",
    "\n",
    "    # Sets the Early Stopping for the model\n",
    "    #early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Training of the model with validation split\n",
    "    history = model.fit(\n",
    "        x=X,\n",
    "        y=y,\n",
    "        epochs=params['epochs'],\n",
    "        batch_size=params['batch_size'],\n",
    "        validation_split=0.25,  # 25% of the data will be used for validation\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate on both training and validation sets\n",
    "    loss, training_accuracy = model.evaluate(X, y, verbose=0)\n",
    "    validation_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "\n",
    "    # Reset of the previous Plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "  # Print the used parameters and both training and validation accuracies\n",
    "    print(f\"Trial {i:>2}: Units={params['units']}, Act: {params['activation']}, Optimizer: {optimizer}, \"\n",
    "          f\"Learning Rate={params['learning_rate']}, Weight Decay={params['weight_decay']}, \"\n",
    "          f\"Epochs={params['epochs']}, Batch Size={params['batch_size']}, \"\n",
    "          f\"Momentum={params['momentum']} Training Accuracy={training_accuracy:.4f}, \"\n",
    "          f\"Validation Accuracy={validation_accuracy:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Print of the Plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross fold validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: Mean Training Accuracy=0.8486, Mean Validation Accuracy=0.7187\n",
      "Hyperparameters: {'units': 4, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 128, 'epochs': 400, 'weight_decay': 0.001, 'momentum': 0.6, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 2: Mean Training Accuracy=0.8609, Mean Validation Accuracy=0.8063\n",
      "Hyperparameters: {'units': 4, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 128, 'epochs': 400, 'weight_decay': 0.002, 'momentum': 0.5, 'activation': 'tanh', 'metrics': 'accuracy'}\n",
      "Trial 3: Mean Training Accuracy=0.8812, Mean Validation Accuracy=0.7907\n",
      "Hyperparameters: {'units': 4, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 64, 'epochs': 400, 'weight_decay': 0.001, 'momentum': 0.6, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 4: Mean Training Accuracy=0.8951, Mean Validation Accuracy=0.8630\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 64, 'epochs': 500, 'weight_decay': 0.002, 'momentum': 0.5, 'activation': 'tanh', 'metrics': 'accuracy'}\n",
      "Trial 5: Mean Training Accuracy=0.7968, Mean Validation Accuracy=0.7570\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 128, 'epochs': 500, 'weight_decay': 0.002, 'momentum': 0.6, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 6: Mean Training Accuracy=0.9255, Mean Validation Accuracy=0.8703\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 64, 'epochs': 300, 'weight_decay': 0.001, 'momentum': 0.5, 'activation': 'tanh', 'metrics': 'accuracy'}\n",
      "Trial 7: Mean Training Accuracy=0.9377, Mean Validation Accuracy=0.8943\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 64, 'epochs': 500, 'weight_decay': 0.002, 'momentum': 0.5, 'activation': 'tanh', 'metrics': 'accuracy'}\n",
      "Trial 8: Mean Training Accuracy=0.8894, Mean Validation Accuracy=0.7973\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 128, 'epochs': 400, 'weight_decay': 0.002, 'momentum': 0.6, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 9: Mean Training Accuracy=0.8870, Mean Validation Accuracy=0.8387\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 128, 'epochs': 300, 'weight_decay': 0.001, 'momentum': 0.6, 'activation': 'tanh', 'metrics': 'accuracy'}\n",
      "Trial 10: Mean Training Accuracy=0.8087, Mean Validation Accuracy=0.7570\n",
      "Hyperparameters: {'units': 4, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 64, 'epochs': 400, 'weight_decay': 0.002, 'momentum': 0.5, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 11: Mean Training Accuracy=0.8447, Mean Validation Accuracy=0.7740\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 64, 'epochs': 300, 'weight_decay': 0.002, 'momentum': 0.6, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 12: Mean Training Accuracy=0.8748, Mean Validation Accuracy=0.8630\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 64, 'epochs': 300, 'weight_decay': 0.001, 'momentum': 0.5, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 13: Mean Training Accuracy=0.9758, Mean Validation Accuracy=0.9440\n",
      "Hyperparameters: {'units': 4, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 64, 'epochs': 400, 'weight_decay': 0.001, 'momentum': 0.5, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Trial 14: Mean Training Accuracy=0.9456, Mean Validation Accuracy=0.8550\n",
      "Hyperparameters: {'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 128, 'epochs': 300, 'weight_decay': 0.002, 'momentum': 0.5, 'activation': 'tanh', 'metrics': 'accuracy'}\n",
      "Trial 15: Mean Training Accuracy=0.9236, Mean Validation Accuracy=0.8700\n",
      "Hyperparameters: {'units': 4, 'optimizer': 'sgd', 'learning_rate': 0.2, 'batch_size': 64, 'epochs': 400, 'weight_decay': 0.002, 'momentum': 0.6, 'activation': 'tanh', 'metrics': 'accuracy'}\n",
      "\n",
      "Overall Mean Training Accuracy: 0.8863\n",
      "Overall Mean Validation Accuracy: 0.8266\n",
      "\n",
      "Best Hyperparameters:\n",
      "Trial: 13\n",
      "Hyperparameters: {'units': 4, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 64, 'epochs': 400, 'weight_decay': 0.001, 'momentum': 0.5, 'activation': 'relu', 'metrics': 'accuracy'}\n",
      "Mean Validation Accuracy: 0.943999993801117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Parameters' space for Grid Search\n",
    "param_space = {\n",
    "    'units': [3, 4],\n",
    "    'optimizer': ['sgd'],\n",
    "    'learning_rate': [0.2, 0.3],\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [300, 400, 500],\n",
    "    'weight_decay': [0.001, 0.002],\n",
    "    'momentum': [0.5, 0.6],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'metrics': ['accuracy']\n",
    "}\n",
    "\n",
    "# Assigning new DataFrame to Data set variable\n",
    "X = df_copy.values\n",
    "\n",
    "# Number of trials with random combinations\n",
    "num_trials = 15\n",
    "\n",
    "# K-fold Cross-validation\n",
    "num_folds = 5\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store training and validation accuracies\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "# Lists to store best hyperparameters\n",
    "best_hyperparameters = {\n",
    "    'trial': 0,\n",
    "    'params': {},\n",
    "    'mean_validation_accuracy': 0\n",
    "}\n",
    "\n",
    "for i in range(num_trials):\n",
    "    # Random parameters\n",
    "    params = {\n",
    "        'units': np.random.choice(param_space['units']),\n",
    "        'optimizer': np.random.choice(param_space['optimizer']),\n",
    "        'learning_rate': np.random.choice(param_space['learning_rate']),\n",
    "        'batch_size': np.random.choice(param_space['batch_size']),\n",
    "        'epochs': np.random.choice(param_space['epochs']),\n",
    "        'weight_decay': np.random.choice(param_space['weight_decay']),\n",
    "        'momentum': np.random.choice(param_space['momentum']),\n",
    "        'activation': np.random.choice(param_space['activation']),\n",
    "        'metrics': np.random.choice(param_space['metrics']),\n",
    "    }\n",
    "\n",
    "    # Lists to store fold-wise accuracies\n",
    "    fold_training_accuracies = []\n",
    "    fold_validation_accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=params['units'], activation=params['activation'], use_bias=True))\n",
    "        model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "        # Set the optimizer with the sampled learning rate\n",
    "        if params['optimizer'] == 'adam':\n",
    "            opt = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "        elif params['optimizer'] == 'sgd':\n",
    "            opt = optimizers.SGD(learning_rate=params['learning_rate'], momentum=params['momentum'])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        # Sets the Loss Function, the Optimizer used in the model, and the Metrics used for evaluation\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[params['metrics']])\n",
    "\n",
    "        # Set the Early Stopping for the model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # Training of the model with validation split and early stopping\n",
    "        history = model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train,\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=params['batch_size'],\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping],  # Add early stopping callback\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Evaluate on both training and validation sets\n",
    "        training_accuracy = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "        validation_accuracy = model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "\n",
    "        fold_training_accuracies.append(training_accuracy)\n",
    "        fold_validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "    # Calculate mean accuracies across folds for this trial\n",
    "    mean_training_accuracy = np.mean(fold_training_accuracies)\n",
    "    mean_validation_accuracy = np.mean(fold_validation_accuracies)\n",
    "\n",
    "    # Store mean accuracies for this trial\n",
    "    training_accuracies.append(mean_training_accuracy)\n",
    "    validation_accuracies.append(mean_validation_accuracy)\n",
    "\n",
    "    # Print mean accuracies and hyperparameters for this trial\n",
    "    print(f\"Trial {i+1}: Mean Training Accuracy={mean_training_accuracy:.4f}, Mean Validation Accuracy={mean_validation_accuracy:.4f}\")\n",
    "    print(\"Hyperparameters:\", params)\n",
    "\n",
    "    # Update best hyperparameters if current trial has higher validation accuracy\n",
    "    if mean_validation_accuracy > best_hyperparameters['mean_validation_accuracy']:\n",
    "        best_hyperparameters['trial'] = i + 1\n",
    "        best_hyperparameters['params'] = params\n",
    "        best_hyperparameters['mean_validation_accuracy'] = mean_validation_accuracy\n",
    "\n",
    "# Print overall mean accuracies\n",
    "print(f\"\\nOverall Mean Training Accuracy: {np.mean(training_accuracies):.4f}\")\n",
    "print(f\"Overall Mean Validation Accuracy: {np.mean(validation_accuracies):.4f}\")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"Trial:\", best_hyperparameters['trial'])\n",
    "print(\"Hyperparameters:\", best_hyperparameters['params'])\n",
    "print(\"Mean Validation Accuracy:\", best_hyperparameters['mean_validation_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4335615038871765\n",
      "0.75\n",
      "0.8199999928474426\n",
      "{'units': 3, 'optimizer': 'sgd', 'learning_rate': 0.3, 'batch_size': 128, 'epochs': 400, 'weight_decay': 0.002, 'momentum': 0.5, 'activation': 'tanh', 'metrics': 'accuracy'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "print(loss)\n",
    "print(validation_accuracy)\n",
    "print(training_accuracy)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
