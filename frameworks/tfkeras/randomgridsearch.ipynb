{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "# Define the search space\n",
    "param_space = {\n",
    "    'units': [3, 4],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'learning_rate': [0.06, 0.05, 0.07, 0.08, 0.1],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 60, 70, 80],\n",
    "    'weight_decay': [0.001, 0.01, 0.002, 0.003],\n",
    "    'momentum': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Number of random combinations to try\n",
    "num_trials = 15\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    # Randomly sample from the search space\n",
    "    params = {\n",
    "        'units': np.random.choice(param_space['units']),\n",
    "        'optimizer': np.random.choice(param_space['optimizer']),\n",
    "        'learning_rate': np.random.choice(param_space['learning_rate']),\n",
    "        'batch_size': np.random.choice(param_space['batch_size']),\n",
    "        'epochs': np.random.choice(param_space['epochs']),\n",
    "        'weight_decay': np.random.choice(param_space['weight_decay']),\n",
    "        'momentum': np.random.choice(param_space['momentum'])\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Build the Keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=params['units'], input_shape=(6,5), activation='relu', kernel_regularizer=regularizers.l2(params['weight_decay'])))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    # Set the optimizer with the sampled learning rate\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    elif params['optimizer'] == 'sgd':\n",
    "        optimizer = optimizers.SGD(learning_rate=params['learning_rate'], momentum=params['momentum'])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_one_hot, y, epochs=params['epochs'],\n",
    "                         batch_size=params['batch_size'], \n",
    "                         validation_split=0.2, \n",
    "                         callbacks=[early_stopping],\n",
    "                           verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_one_hot, y, verbose=0)\n",
    "\n",
    "    print(f\"Trial {_:>2}: Units={params['units']}, Momentum={params['momentum']}, Epochs={params['epochs']}, Decay={params['weight_decay']}, Optimizer={params['optimizer']}, Learning Rate={params['learning_rate']:.3f}, Batch Size={params['batch_size']}, Accuracy={accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
