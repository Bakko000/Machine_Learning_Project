{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id   input1               input2 input3                      input4  \\\n",
      "0  #     Data                  Set  (v2):                     3D-2023   \n",
      "1  #      Nov                 2023      -                     ML-2023   \n",
      "2  #    INFO:  micheli@di.unipi.it      -  lorenzo.simone@di.unipi.it   \n",
      "3  #      (C)                 CIML  group                           -   \n",
      "4  #  Format:                  NaN    NaN                         NaN   \n",
      "\n",
      "    input5 input6 input7 input8  input9 input10  input11  target_x  target_y  \\\n",
      "0      NaN    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "1      CUP    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "2      NaN    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "3  Micheli   2023    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "4      NaN    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "\n",
      "   target_z  \n",
      "0       NaN  \n",
      "1       NaN  \n",
      "2       NaN  \n",
      "3       NaN  \n",
      "4       NaN  \n",
      "  id   input1               input2 input3                      input4  \\\n",
      "0  #     Data                  Set  (v2):                     3D-2023   \n",
      "1  #      Nov                 2023      -                     ML-2023   \n",
      "2  #    INFO:  micheli@di.unipi.it      -  lorenzo.simone@di.unipi.it   \n",
      "3  #      (C)                 CIML  group                           -   \n",
      "4  #  Format:                  NaN    NaN                         NaN   \n",
      "\n",
      "    input5 input6 input7 input8  input9 input10  input11  target_x  target_y  \\\n",
      "0      NaN    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "1      CUP    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "2      NaN    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "3  Micheli   2023    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "4      NaN    NaN    NaN    NaN     NaN     NaN      NaN       NaN       NaN   \n",
      "\n",
      "   target_z  \n",
      "0       NaN  \n",
      "1       NaN  \n",
      "2       NaN  \n",
      "3       NaN  \n",
      "4       NaN  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from api.keras.binary_nn import BinaryNN\n",
    "from api.data_handler import DataHandler\n",
    "\n",
    "# Creation of a DataHandler Object\n",
    "data_handler = DataHandler(['id', 'input1', 'input2', 'input3', 'input4', 'input5', 'input6', 'input7', 'input8', 'input9', 'input10', 'input11', 'target_x', 'target_y', 'target_z'])\n",
    "                                #ID, INPUTS, TARGET_x, TARGET_y, TARGET_z\n",
    "# Number of different Datasets\n",
    "datasets_number = 1\n",
    "\n",
    "# Lists of DataFrames\n",
    "df_train : list[pd.DataFrame] = []\n",
    "df_test  : list[pd.DataFrame] = []\n",
    "\n",
    "# Load the Training/Test sets into pandas DataFrames\n",
    "for i in range(datasets_number):\n",
    "    df_train.append(data_handler.load_data(f'data/cup/ML-CUP23-TR.csv'))\n",
    "    df_test.append(data_handler.load_data(f'data/cup/ML-CUP23-TS.csv'))\n",
    "\n",
    "    # Print the head of the loaded data\n",
    "    print(df_train[i].head())\n",
    "    print(df_test[i].head())\n",
    "\n",
    "#nn_i = BinaryNN(params=params, monk_i=dataset_i+1, trial=trial+1)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.9172796   -0.7127266   -0.9899035    0.9928187    0.9936488\n",
      "   0.995543     0.7110739    0.40764457  -0.68854785   0.6168897\n",
      "   7.897453   -35.936382    21.077147  ]\n"
     ]
    }
   ],
   "source": [
    "dataset = loadtxt(\"data/cup/ML-CUP23-TR.csv\", delimiter=',', usecols=range(1, 14), dtype=np.float64)\n",
    "print(dataset[0]) # check the correctness\n",
    "      \n",
    "\n",
    "# Build training set\n",
    "x = dataset[:, :-3]\n",
    "y = dataset[:, -3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 800\n",
      "Validation set size: 100\n",
      "Internal Test set size: 100\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the proportions for train, validation, and test sets\n",
    "train_percent = 0.8\n",
    "val_percent = 0.1\n",
    "test_percent = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_samples = len(dataset)\n",
    "num_train = int(train_percent * num_samples)\n",
    "num_val = int(val_percent * num_samples)\n",
    "num_test = int(test_percent * num_samples)\n",
    "\n",
    "# Shuffle the indices\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split the indices into train, validation, and test sets\n",
    "train_indices = indices[:num_train]\n",
    "val_indices = indices[num_train:num_train + num_val]\n",
    "test_indices = indices[num_train + num_val:]\n",
    "\n",
    "# Use the indices to get the corresponding data for each set\n",
    "x_train, y_train = x[train_indices], y[train_indices]\n",
    "x_val, y_val = x[val_indices], y[val_indices]\n",
    "x_test, y_test = x[test_indices], y[test_indices]\n",
    "\n",
    "# Print the sizes of the resulting sets\n",
    "print(\"Train set size:\", len(x_train))\n",
    "print(\"Validation set size:\", len(x_val))\n",
    "print(\"Internal Test set size:\", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters' space for Grid Search (1 for each Dataset)\n",
    "param_space = {\n",
    "    0: {\n",
    "        'input_units': [17],\n",
    "        'hidden_units': [2, 3, 4, 5],\n",
    "        'patience': [10,100,200],\n",
    "        'factor_lr_dec': [0.5, 1],\n",
    "        'step_decay': [500, 1000, 1500],\n",
    "        'learning_rate': [float(i/10) for i in range(1,10)] + [float(i/100) for i in range(1,10)] + [0.99, 0.999],\n",
    "        'batch_size': [7, 8, 9, 15, 16, 17, 31, 32, 33, 62, 63, 64, 65],\n",
    "        'epochs': [int(350+epochs) for epochs in range(0,50,10)],\n",
    "        'weight_decay': [float(i/10) for i in range(1,10)] + [0.01, 0.001, 0.0001],\n",
    "        'weight_init': ['glorot_normal', 'lecun_normal', 'he_normal', 'he_uniform'],\n",
    "        'momentum': [float(i/100) for i in range(1,9)] + [float(i/10) for i in range(1,9)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'output_activation': ['linear'],\n",
    "        'metrics': ['mean_squared_error'],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'continuous-multioutput' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m nn_i \u001b[38;5;241m=\u001b[39m BinaryNN(params\u001b[38;5;241m=\u001b[39mparams, monk_i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, trial\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# For each K-fold returns the indexes of the data splitted in: <X_train,y_train> and <X_val,y_val>\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, val_index \u001b[38;5;129;01min\u001b[39;00m kfold\u001b[38;5;241m.\u001b[39msplit(x_train, y_train):\n\u001b[0;32m     27\u001b[0m     x_kfold_train, x_kfold_val \u001b[38;5;241m=\u001b[39m x_train[train_index], y_train[val_index]\n\u001b[0;32m     28\u001b[0m     y_kfold_train, y_kfold_val \u001b[38;5;241m=\u001b[39m y_train[train_index], y_train[val_index]\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:377\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         (\n\u001b[0;32m    372\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    375\u001b[0m     )\n\u001b[1;32m--> 377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:108\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    107\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[0;32m    109\u001b[0m     train_index \u001b[38;5;241m=\u001b[39m indices[np\u001b[38;5;241m.\u001b[39mlogical_not(test_index)]\n\u001b[0;32m    110\u001b[0m     test_index \u001b[38;5;241m=\u001b[39m indices[test_index]\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:758\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 758\u001b[0m     test_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m test_folds \u001b[38;5;241m==\u001b[39m i\n",
      "File \u001b[1;32mc:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:701\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    699\u001b[0m allowed_target_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_of_target_y \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_target_types:\n\u001b[1;32m--> 701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported target types are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    703\u001b[0m             allowed_target_types, type_of_target_y\n\u001b[0;32m    704\u001b[0m         )\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y)\n\u001b[0;32m    709\u001b[0m _, y_idx, y_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'continuous-multioutput' instead."
     ]
    }
   ],
   "source": [
    "# Creation of a BinaryNN objct for each dataset\n",
    "nn: list[BinaryNN] = []\n",
    "\n",
    "# Different values per dataset\n",
    "trials_list = 30\n",
    "k_values = 5\n",
    "n_hidden_layers_list = 1\n",
    "\n",
    "# Search of the best Hyperparameters to each Training set\n",
    "k = k_values\n",
    "# K-fold Cross-validation\n",
    "kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # For each iteration we choose the hyperparameters (randomly) and we use them with K-fold CV\n",
    "for i in range(trials_list):\n",
    "        #if i != 2:\n",
    "        #    continue\n",
    "        \n",
    "        # Random parameters\n",
    "        params = data_handler.random_dictionary(param_space[0])\n",
    "\n",
    "        # Creation of the Neural Network object\n",
    "        nn_i = BinaryNN(params=params, monk_i=1, trial=i+1)\n",
    "\n",
    "        # For each K-fold returns the indexes of the data splitted in: <X_train,y_train> and <X_val,y_val>\n",
    "        for train_index, val_index in kfold.split(x_train, y_train):\n",
    "            x_kfold_train, x_kfold_val = x_train[train_index], y_train[val_index]\n",
    "            y_kfold_train, y_kfold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            # Building the model\n",
    "            nn_i.create_model(n_hidden_layers=1)\n",
    "\n",
    "            # Training the model\n",
    "            nn_i.fit(\n",
    "                x_train=x_kfold_train,\n",
    "                y_train=y_kfold_train,\n",
    "                x_val=x_kfold_val,\n",
    "                y_val=y_kfold_val\n",
    "            )\n",
    "\n",
    "            # Evaluating the model\n",
    "            nn_i.evaluate(\n",
    "                x_train=x_kfold_train,\n",
    "                y_train=y_kfold_train,\n",
    "                x_val=x_kfold_val,\n",
    "                y_val=y_kfold_val\n",
    "            )\n",
    "\n",
    "        # Case of first append\n",
    "        #if len(nn) == dataset_i:\n",
    "        #    nn.append(nn_i)\n",
    "        \n",
    "        # Print the results of this trial\n",
    "        print(\"------------------ Current Hyperparameters ------------------\")\n",
    "        nn_i.print_training_info()\n",
    "        print(\"-------------------- Best Hyperparameters -------------------\")\n",
    "        nn[0].print_training_info()\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # Update best hyperparameters if: no high overfitting AND (higher mean VL accuracy OR (equal mean AND\n",
    "        if nn_i.mean_tr_accuracy-0.1 <= nn_i.mean_vl_accuracy \\\n",
    "            and (\n",
    "                    nn[0].mean_vl_accuracy < nn_i.mean_vl_accuracy \\\n",
    "                or (\n",
    "                    nn[0].mean_vl_accuracy == nn_i.mean_vl_accuracy and nn[0].mean_tr_accuracy < nn_i.mean_tr_accuracy\n",
    "                    )\n",
    "            ):\n",
    "            nn[0] = nn_i\n",
    "        \n",
    "        # Case of TR/VL accuracy = 1.0 AND TR/VL loss minor\n",
    "        if nn_i.mean_tr_accuracy == 1 and nn_i.mean_vl_accuracy == 1 \\\n",
    "            and nn_i.mean_tr_accuracy == nn[0].mean_tr_accuracy \\\n",
    "            and nn_i.mean_vl_accuracy == nn[0].mean_vl_accuracy \\\n",
    "            and abs(nn_i.mean_tr_accuracy - nn_i.mean_vl_accuracy) < 0.02 \\\n",
    "            and nn_i.mean_vl_loss < nn[0].mean_vl_loss \\\n",
    "            and nn_i.mean_tr_loss < nn[0].mean_tr_loss:\n",
    "            nn[0] = nn_i\n",
    "        \n",
    "        # Exit case\n",
    "        if nn_i.mean_tr_accuracy == 1 and nn_i.mean_vl_accuracy == 1 \\\n",
    "            and nn_i.mean_vl_loss < 0.1 and nn_i.mean_tr_loss < 0.1 \\\n",
    "            and abs(nn_i.mean_vl_loss - nn_i.mean_tr_loss) < 0.01:\n",
    "            nn[0] = nn_i\n",
    "            break\n",
    "\n",
    "# Print output\n",
    "print(f\"### Best Hyperparameters of Monk {i+1} ###\")\n",
    "nn[0].print_training_info()\n",
    "print(\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
